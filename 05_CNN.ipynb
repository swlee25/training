{"cells":[{"cell_type":"markdown","metadata":{"id":"zUvWosZg8gG2"},"source":["# Hands-on Session 5: Convolutional Neural Network\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U21gJds82ljj"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6vd4RaoBwAx"},"outputs":[],"source":["import tensorflow as tf\n","#tf.logging.set_verbosity(tf.logging.ERROR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EnJCNZuuzwQq"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n"]},{"cell_type":"markdown","metadata":{"id":"tQFzmfAUk1T_"},"source":["### Fruits-360 Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kj_RLGkKxUiL"},"outputs":[],"source":["!git clone https://github.com/Horea94/Fruit-Images-Dataset.git"]},{"cell_type":"markdown","metadata":{"id":"p_ihozQwk-XF"},"source":["Let's start by doing some preparation on the data and sort out the labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TiAIamXyTtN"},"outputs":[],"source":["ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_ZO-sfByCpf"},"outputs":[],"source":["import glob\n","fruitpath = glob.glob(\"/content/Fruit-Images-Dataset/Training/*/*.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HA6GZ4fSy8SJ"},"outputs":[],"source":["print(fruitpath[0:5])\n","print(\"\")\n","print(len(fruitpath))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OB6WM2wlzQ5y"},"outputs":[],"source":["import random\n","random.seed(42)\n","random.shuffle(fruitpath)\n","print(fruitpath[0:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-IaimyQz3D1"},"outputs":[],"source":["import cv2\n","import os\n","\n","print(\"GETTING DATA & LABELS...\")\n","data = []\n","labels = []\n","for imgPath in fruitpath:\n","  image = cv2.imread(imgPath)\n","  image = cv2.resize(image, (32, 32)).flatten()  # why do we do this?\n","  data.append(image)\n","  labels.append(imgPath.split(os.path.sep)[-2])\n","\n","print(len(data))\n","print(len(labels))\n","\n","# scale the raw pixel intensities to the range [0, 1]\n","print(\"PREPARING...\")\n","data = np.array(data, dtype=\"float\") / 255.0\n","labels = np.array(labels)\n","\n","print(\"DONE!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRzv6CjWM4LT"},"outputs":[],"source":["print(np.unique(labels))\n","print(len(np.unique(labels)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Or8U92Fm1Tof"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","\n","# partition the data into training and testing splits using 75% of\n","# the data for training and the remaining 25% for testing\n","(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42, stratify=labels)\n","print(trainX.shape)\n","print(testX.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTNRs5rl20Ep"},"outputs":[],"source":["\n","from sklearn.preprocessing import LabelBinarizer\n","\n","\n","# convert the labels from integers to one-hot-encoded vectors (for 2-class\n","# binary classification you should use Keras' to_categorical function\n","# instead as the scikit-learn's LabelBinarizer will not return a\n","# vector)\n","lb = LabelBinarizer()\n","trainY = lb.fit_transform(trainY)\n","testY = lb.transform(testY)\n","\n","# show one example\n","print(trainY[0])\n","print(testY[0])"]},{"cell_type":"markdown","metadata":{"id":"VGYtPbxClar2"},"source":["Before we get to the CNN, let's build a simple MLP, which means that our input is a single dimension of values. The only way to transform an image (2D) into a vector (1D), is by \"flattening\" it. Flattening typically takes each row of the image and strings them together in order. Considering color images (3D), flattening would mean the process is repeated for all three channels.\n","\n","A 32x32 RGB image will result in 32 * 32 * 3 = 3,072 values after flattening."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctVM84xQ231Z"},"outputs":[],"source":["from keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","\n","# define the 3072-512-512 architecture using Keras\n","model = Sequential()\n","model.add(Dense(512, input_shape=(3072,), activation=\"sigmoid\"))\n","model.add(Dense(512, activation=\"sigmoid\"))\n","model.add(Dense(len(lb.classes_), activation=\"softmax\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLiOjt5mCHkL"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YeBgPSYdCNBF"},"outputs":[],"source":["# initialize parameters\n","initial_lr = 0.01\n","EPOCHS = 50\n","BATCH_SIZE = 64\n","opt = SGD(learning_rate=initial_lr)\n","\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2A5G__E1CuN5"},"outputs":[],"source":["# train the neural network\n","H_mlp = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=EPOCHS, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHe1_LB5eGXU"},"outputs":[],"source":["from sklearn.metrics import classification_report\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KvAFAajGl_s"},"outputs":[],"source":["# evaluate the network\n","print(\"EVALUATING NETWORK...\")\n","predictions = model.predict(testX, batch_size=32)\n","print(classification_report(testY.argmax(axis=1),\n","predictions.argmax(axis=1), target_names=lb.classes_))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxQQO2E7nibC"},"outputs":[],"source":["# Write a function to plot both training loss and accuracy\n","\n","def plotcurves(H_mlp, EPOCHS):\n","\tN = np.arange(0, EPOCHS)\n","\tplt.figure(figsize=(12,4))\n","\tplt.subplot(121)\n","\tplt.plot(N, H_mlp.history[\"loss\"], label=\"train_loss\")\n","\tplt.plot(N, H_mlp.history[\"val_loss\"], label=\"val_loss\")\n","\tplt.xlabel(\"Epoch #\")\n","\tplt.ylabel(\"Loss\")\n","\tplt.legend()\n","\n","\tplt.subplot(122)\n","\tplt.plot(N, H_mlp.history[\"accuracy\"], label=\"train_acc\")\n","\tplt.plot(N, H_mlp.history[\"val_accuracy\"], label=\"val_acc\")\n","\tplt.xlabel(\"Epoch #\")\n","\tplt.ylabel(\"Accuracy\")\n","\tplt.legend()\n","\tplt.tight_layout()\n","\n","plotcurves(H_mlp, EPOCHS)"]},{"cell_type":"markdown","metadata":{"id":"lsYYwnVvzSgM"},"source":["**Question:** Has the model converged? If it hasn't converge, what can be done to improve the convergence?"]},{"cell_type":"markdown","metadata":{"id":"yzFx0z62mGSU"},"source":["## Building a CNN model\n","\n","Now, let's get to the CNN. We use Keras `Sequential` model to build the entire network structure incrementally or in \"sequential\" manner, by \"adding\" layers from scratch up. The other is the `Model` model, which we will encounter later.\n","\n","Notice one big difference here compared to the MLP? We now put the entire RGB (3-dim) image into the CNN instead of just it flattened into a 1-dim input vector for MLP case. The structure of CNN allows for this since convolutions will be performed on to the image cube itself.\n","\n","*Note: Dropouts are removed for now, but you can put them back in later to try*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24l6ex5PRwKo"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","\n","# input image dimensions\n","w, h = 32, 32\n","\n","cnn_model=Sequential()\n","\n","# Feature Extraction layers: Conv layers\n","cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(w,h,3)))\n","cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n","cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# Convert 2D features to 1D Vector for classication\n","cnn_model.add(Flatten())\n","\n","# Classification layers: FC/Dense layers\n","cnn_model.add(Dense(64, activation='sigmoid'))\n","cnn_model.add(Dense(len(lb.classes_), activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWGAs-q8TZPu"},"outputs":[],"source":["cnn_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"WT3moUIWnWFs"},"source":["Because of the change in input shape, we need to reshape the train and test data into the expected form. The labels remain the same."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LVbRfEXT1Xr"},"outputs":[],"source":["cnn_model.compile(loss='categorical_crossentropy',\n","              optimizer='sgd',\n","              metrics=['accuracy'])\n","\n","trainXb = np.reshape(trainX, (trainX.shape[0], w, h, 3))\n","testXb = np.reshape(testX, (testX.shape[0], w, h, 3))\n","\n","lb = LabelBinarizer()\n","trainY = lb.fit_transform(trainY)\n","testY = lb.transform(testY)\n","\n","EPOCHS = 30\n","H_cnn = cnn_model.fit(trainXb, trainY, validation_data=(testXb, testY),\n","          batch_size=BATCH_SIZE,\n","          epochs=EPOCHS,\n","          verbose=1)\n","\n","score = cnn_model.evaluate(testXb, testY, verbose=0)\n","\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJk58eIUnbsJ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plotcurves(H_cnn, EPOCHS)"]},{"cell_type":"markdown","metadata":{"id":"6LBe_wWNL6-z"},"source":["**Exercise:**\n","What do you think of the performance of the model? From the loss plot, do you think it has it converged? If not, you can increase the epoch.\n","\n","Try the following and see how these changes can affect the performance:\n","\n","1) Change the **activation function** at the fully-connected (FC) layers from *sigmoid* to *ReLU*.\n","\n","2) Change the **optimizer**; e.g. *adam*, *rmsprop*.\n","\n","3) Change the **architecture** by adding more convolutional or fully-connected layers, change number of filters and filter size for each conv layer."]},{"cell_type":"markdown","metadata":{"id":"bDlh86vwn6wO"},"source":["For something extra, let's show some of the images in our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ZnazISpDbm7"},"outputs":[],"source":["from google.colab.patches import cv2_imshow\n","\n","start = 2000\n","fruittray = []\n","for i in range(20):\n","  img = np.reshape(data[start+i]*255, (w, h, 3))\n","  fruittray.append(img)\n","\n","ft = np.hstack(fruittray)\n","cv2_imshow(ft)\n"]},{"cell_type":"markdown","metadata":{"id":"XYNFnKgZbtBH"},"source":["### Saving model and weights\n","\n","Since training a CNN could take a long time, it is good to know how to save the essential parts of it. The most reliable way to do this is to save up the 'model' and the 'weights' in separate files. The model contains the definition of the the CNN structure and architecture, while the weights contain its corresponding weight values only. To deploy the network again, you need both parts to work.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJSBB7OENTzd"},"outputs":[],"source":["%whos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llJoHyI_bst1"},"outputs":[],"source":["from keras.models import model_from_json\n","\n","# serialize model to JSON\n","model_json = cnn_model.to_json()\n","with open(\"fruits-model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","\n","# serialize weights to HDF5\n","cnn_model.save_weights(\"fruits-model.weights.h5\")\n","print(\"Saved model to disk\")"]},{"cell_type":"markdown","metadata":{"id":"wZAjg0Z5oNm3"},"source":["Use some linux command to check on their file sizes.... (modify accordingly)"]},{"cell_type":"markdown","metadata":{"id":"OLmDBEI5cThv"},"source":["### Load models and weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9Zev_AfcTM6"},"outputs":[],"source":["# load json and create model\n","# json_file = open('fruits-model.json', 'r')\n","# loaded_model_json = json_file.read()\n","# json_file.close()\n","\n","from keras.initializers import glorot_uniform\n","#Reading the model from JSON file\n","with open('fruits-model.json', 'r') as json_file:\n","    loaded_model_json= json_file.read()\n","\n","loaded_model = model_from_json(loaded_model_json)\n","loaded_model.summary()\n","\n","loaded_model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UeadfYPppjES"},"outputs":[],"source":["# load weights into new model\n","loaded_model.load_weights(\"fruits-model.weights.h5\")\n","print(\"Loaded model from disk\")\n","\n","loaded_model.compile(loss='categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n04l2OE6-lTg"},"source":["### Perform prediction with the loaded model"]},{"cell_type":"markdown","metadata":{"id":"1TzgBMHhB11K"},"source":["**Q**: Noticed that we used image from the 'Training' folder images, for both training and validation! But this is fine, because we can now use the 'Test' folder images purely for evaluation of unseen samples. So, complete the code below to perform evaluation on the 'Test' images and see how well your model performs."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5xd9tYD7iJUy","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["testpath = glob.glob(\"/content/Fruit-Images-Dataset/Test/*/*.jpg\")\n","\n","testdata = []\n","testlabels = []\n","for imgPath in testpath:\n","  image = cv2.imread(imgPath)\n","  image = cv2.resize(image, (32, 32))\n","  testdata.append(image)\n","  testlabels.append(imgPath.split(os.path.sep)[-2])\n","\n","print(len(testdata))\n","print(len(testlabels))\n","print(len(np.unique(testlabels)))\n","\n","# create one-hot encoding labels\n","\n","\n","# evaluate the model by performing prediction\n","\n","\n","\n","print('Test loss:', testscore[0])\n","print('Test accuracy:', testscore[1])\n"]},{"cell_type":"markdown","metadata":{"id":"EhfZF9V9ysSE"},"source":["Let's save out of Colab our best model and weights for future use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbTvchaRyrTV"},"outputs":[],"source":["from google.colab import files\n","files.download( \"fruits-model.weights.h5\" )\n","files.download( \"fruits-model.json\" )"]},{"cell_type":"markdown","source":["### Try it in Streamlit\n","\n","Place the model and weights into your working directory and run the streamlit `cnn_demo.py`. <br>\n","Upload some images to try out your model prediction."],"metadata":{"id":"h0T9y5eQ2Zry"}},{"cell_type":"markdown","metadata":{"id":"5-1Prd56iCFG"},"source":["## Using Pre-trained models\n","\n","### Dog and Cat Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V53glgrdiFti"},"outputs":[],"source":["!wget -c https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPbtcF7vlyK_"},"outputs":[],"source":["!unzip -q Cat_Dog_data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBu9p5rjoMik"},"outputs":[],"source":["%cd /content/Cat_Dog_data"]},{"cell_type":"markdown","metadata":{"id":"AgHeNvTPzYrR"},"source":["Let's get a random dog picture from outside the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNMLhi5vzm7N"},"outputs":[],"source":["# load the class labels from disk\n","\n","rows = open('/content/gdrive/My Drive/DreamCatcher/PAI(Jan2025)/Day2/synset_words.txt').read().strip().split(\"\\n\")\n","classes = [r[r.find(\" \") + 1:].split(\",\")[0] for r in rows]\n","\n","# print out ImageNet classes\n","print(len(classes))\n","print(classes)"]},{"cell_type":"markdown","metadata":{"id":"DVoZuaOozkj5"},"source":["Let's use one of the most robust CNN models around: The ResNet50 (50-layer deep residual network), which has already been pre-trained on ImageNet (millions of natural images) on 1,000 categories. It is unlikely that any of us would want waste resources to re-train again a large network involving massive amounts of images, so we can use these \"off-the-shelf\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUQioNsg0AVf"},"outputs":[],"source":["from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n","from keras.models import load_model\n","\n","model = ResNet50(weights='imagenet', include_top=True)\n","\n","img_path = '/content/gdrive/My Drive/DreamCatcher/PAI(Jan2025)/Day2/dog.jpg'\n","\n","img = image.load_img(img_path, target_size=(224, 224))\n","x = image.img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","x = preprocess_input(x)\n","\n","preds = model.predict(x)\n","# decode the results into a list of tuples (class, description, probability)\n","# (one such list for each sample in the batch)\n","print('Predicted:\\n', decode_predictions(preds, top=5)[0])"]},{"cell_type":"markdown","metadata":{"id":"lXwihQ9-0-19"},"source":["The results are pretty accurate. The dog is not only correctly classified as a 'dog', but since ImageNet contains fine level categories including different breeds of dogs, it is able to know this is a 'laborador retriever'.\n","\n","Using ImageNet pre-trained models is a good starting point towards practical deep learning. If your category is among ImageNet's 1K categories, you could very well safely use a pre-trained model. But, if you are interested to adapt the model to suit your categories, you can fine-tune the model further."]},{"cell_type":"markdown","metadata":{"id":"VoMAg1ijoTc9"},"source":["# Transfer Learning\n","\n","### ImageDataGenerator\n","\n","Keras has a ImageDataGenerator which allows us to prepare a directory structure based on the partitioned data sets. The data should be placed into 2 different folders named as “train” and “test”. The train folder should contain ‘n’ folders each containing images of respective classes. For example, In the Dog vs Cats data set, the \"train\" folder should have 2 folders, namely “Dog” and “Cat” containing respective images inside them. Technically, images in the test folder would be in only a single directory because they will be used for prediction (no need class distinction for training). However, we normally also need to set aside a validation set to test our model, organized similarly to the train set.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/800/1*HpvpA9pBJXKxaPCl5tKnLg.jpeg\" width=400 />\n","\n","The folder names for the classes are important, name them consistently (for both train and validation sets) with respective label names so that it would be easy for you later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"anxqPANik5uk"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","\n","# change this according to your system RAM or GPU VRAM\n","train_batchsize = 100\n","val_batchsize = 10\n","\n","# Mobilenet_V2\n","train_datagen = ImageDataGenerator(\n","        preprocessing_function=preprocess_input\n","        #rescale=1./255\n","        )\n","\n","train_generator = train_datagen.flow_from_directory(\n","    directory=r\"train/\",\n","    target_size=(224, 224),\n","    color_mode=\"rgb\",\n","    batch_size=train_batchsize,\n","    class_mode=\"categorical\",\n","    shuffle=True,\n","    seed=0\n",")\n","\n","valid_datagen = ImageDataGenerator(\n","        preprocessing_function=preprocess_input\n","        #rescale=1./255\n","        )\n","\n","valid_generator = valid_datagen.flow_from_directory(\n","    directory=r\"test/\",\n","    target_size=(224, 224),\n","    color_mode=\"rgb\",\n","    batch_size=val_batchsize,\n","    class_mode=\"categorical\",\n","    shuffle=False,\n","    seed=0\n",")"]},{"cell_type":"markdown","metadata":{"id":"zVz509fuqc6x"},"source":["### Using base CNN models from Keras\n","\n","Keras Applications contain a small selection of deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n","\n","**Getting models with and without the 'top'**\n","\n","Most of these models are a series of convolutional layers followed by one or a few dense (or fully connected) layers. So, they come with two flavours -- one with and one without the 'top', that is the \"top\" or the classifier end of the network.\n","\n","`include_top` lets you select if you want the final dense layers or not.\n","\n","- Convolutional layers work as feature extractors. They identify a series of patterns in the image, and each layer can identify more elaborate patterns by seeing patterns of patterns.\n","- Dense layers are capable of interpreting the found patterns in order to classify: this image contains cats, dogs, cars, etc. They can be perceived as part of a classifier (like a MLP).\n","\n","The weights in a Dense layer are totally dependent on the input size (in this case, input to the Dense layer is the flattened last conv layer). It's one weight per element of the input. So your input is required to be always of a consistent same size, or else you will not be able to process further.\n","\n","Hence, removing the final Dense layer (technically, its the activation connecting to the output) allows you to define the input size. The network can be **retrained with your own choice and design of Dense layers** based on the specific task you are working on."]},{"cell_type":"markdown","metadata":{"id":"WA0qVw0j8-pZ"},"source":["### Fine-tuning a pre-trained model for your task\n","\n","For the purpose of this training, we shall use a slim CNN architecture called MobileNetV2 instead of larger, deeper ones (VGG16, ResNet50). For the complete list of Keras pretrained models, see [here](https://keras.io/api/applications/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05Timi2Jsoow"},"outputs":[],"source":["# load base MobileNetV2 pre-trained model (from Keras)\n","base_model = MobileNetV2(include_top=False, weights='imagenet', pooling='avg', input_shape=(224, 224, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16JLTbgepdRO"},"outputs":[],"source":["base_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"poOgd0kg8gIv"},"source":["#### Practical advice on fine-tuning\n","\n","Fine-tuning is a way of adapting an existing pre-trained model to a different domain (that means, *different image data*, or *same type of data but different dataset*). Typically, the task is similar. If you are adapting an ImageNet-trained model (which was built for the purpose of image classification), chances are that you intend to use it for another image classification problem. Using it for a different kind of data such as text or speech would usually not work because the model may not be expecting the same shape or dimension of data.  \n","\n","For example, if you are planning to fine-tune GoogLeNet model (trained on ImageNet data) for flower classification, it should be no issue. However, if you are planning to fine-tune GoogLeNet model for face identification, it would be a less suitable fit but still acceptable. If you want to fine-tune it for speech recognition, it would most likely be impossible for the architecture is not meant for that task.\n","\n","Practically, fine-tuning is a quick way of creating a new model for your new domain...\n","- if you do not have the necessary computational resources to train from scratch on your data.\n","- if the amount of data in your new domain may not be sufficient to perform a full training (from scratch). In other words, your new dataset is much smaller than ImageNet's millions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSysIx8bt1o2"},"outputs":[],"source":["from tensorflow.keras.layers import Lambda\n","from keras import models\n","from keras import layers\n","\n","# Create the model\n","model = models.Sequential()\n","model.add(Lambda(lambda x: x, input_shape=(224, 224, 3)))\n","\n","# Add the resnet/vgg/mobilenet convolutional base model\n","model.add(base_model)\n","\n","# Add the fully-connected layers\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(2, activation='softmax'))\n","\n","model.summary()\n","\n","model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCTgfH1n7NMc"},"outputs":[],"source":["from tensorflow.keras.layers import Lambda\n","from keras import models\n","from keras import layers\n","\n","# Create the model\n","model = models.Sequential()\n","model.add(Lambda(lambda x: x, input_shape=(224, 224, 3)))\n","# Add the resnet/vgg/mobilnet convolutional base model\n","model.add(base_model)\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(2, activation='softmax'))\n","\n","# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all convolutional layers\n","#for layer in base_model.layers[:-4]:\n","#    layer.trainable = False\n","\n","# Check the trainable status of the individual layers\n","#for layer in base_model.layers:\n","#    print(layer, layer.trainable)\n","\n","model.summary()\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OAyjkyaReQ-"},"outputs":[],"source":["H = model.fit(train_generator,\n","        epochs=10,   # use a bigger number to train longer\n","        validation_data=valid_generator\n",")\n","\n","model.save_weights('CNN_dog.weights.h5')  # always save your weights after training or during training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bBJXwAYT_qET"},"outputs":[],"source":["# Plot the accuracy and loss curves\n","acc = H.history['accuracy']\n","val_acc = H.history['val_accuracy']\n","loss = H.history['loss']\n","val_loss = H.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'b', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUx3wMRmNBME"},"outputs":[],"source":["# Create a generator for prediction\n","validation_dir = r\"test/\"\n","validation_generator = valid_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(224, 224),\n","        batch_size=val_batchsize,\n","        class_mode='categorical',\n","        shuffle=False)\n","\n","# Get the filenames from the generator\n","fnames = valid_generator.filenames\n","\n","# Get the ground truth from generator\n","ground_truth = valid_generator.classes\n","\n","# Get the label to class mapping from the generator\n","label2index = valid_generator.class_indices\n","\n","# Getting the mapping from class index to class label\n","idx2label = dict((v,k) for k,v in label2index.items())\n","\n","# Get the predictions from the model using the generator\n","predictions = model.predict(valid_generator, verbose=1)\n","predicted_classes = np.argmax(predictions,axis=1)\n","\n","errors = np.where(predicted_classes != ground_truth)[0]\n","print(\"No of errors = {}/{}\".format(len(errors),valid_generator.samples))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h852WFv5UJhS"},"outputs":[],"source":["# Show the incorrectly predicted images (first ten only)\n","for i in range(10):\n","    pred_class = np.argmax(predictions[errors[i]])\n","    pred_label = idx2label[pred_class]\n","\n","    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n","        fnames[errors[i]].split('/')[0],\n","        pred_label,\n","        predictions[errors[i]][pred_class])\n","\n","    original = image.load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n","    plt.figure(figsize=[5,5])\n","    plt.axis('off')\n","    plt.title(title)\n","    plt.imshow(original)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"mq340B1bpApX"},"source":["**Q**: What do you think of the performance of the model? What can be improved? What can be tested out?\n","\n","Try the following:\n","* Use a different base network model (e.g. ResNet50, VGG16, Densenet) provided by Keras (https://keras.io/api/applications/)\n","* Modify the top (FC/Dense) layers\n","* Increase batch size\n","* Good to view the loss/accuracy plots to have a better idea if your network is underfitting or overfitting.\n","* Train more than just the final top layers (currently all weights from base model are frozen, no change allowed). Allow maybe the last few blocks to be trained as well."]},{"cell_type":"markdown","metadata":{"id":"H511-wZ8MGlm"},"source":["To freeze more layers for fine-tuning, you just need to alter the 'trainable' property. Leaving them turned on (True) indicates that they will be fine-tuned."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8ZvpodMMFiX"},"outputs":[],"source":["# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all convolutional layers\n","for layer in base_model.layers[:-4]:\n","    layer.trainable = False\n","\n","# Check the trainable status of the individual layers\n","for layer in base_model.layers:\n","    print(layer, layer.trainable)\n"]},{"cell_type":"markdown","metadata":{"id":"hD8n2vJuLmDV"},"source":["# Early Stopping and Model Checkpoint\n","\n","Choosing the number of training epochs to use in a neural network is not an easy task. Too many epochs can lead to overfitting the training dataset while too few may result in an underfit model. A way to solve this poblem is using **Early stopping callback**, a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.\n","\n","The EarlyStopping callback will stop training once triggered, but the model at the end of training may not be the model with best performance on the validation dataset. An additional callback that will save the best model observed during training for later use  is required. This is the **ModelCheckpoint callback**, which we can use it to save the best model observed during training as defined by a chosen performance measure on the validation dataset.\n","\n","https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\n","\n","The following are the sample codes for adding the callbacks for training the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9Ap5sSdLvZs"},"outputs":[],"source":["# This code is not for standalone\n","\n","# simple early stopping and model checkpointing\n","tenses = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n","mc = tf.keras.callbacks.ModelCheckpoint('best_model.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","# set the callbacks when fitting the model\n","history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=1000, verbose=0, callbacks=[es, mc])\n","\n","# load the saved model\n","saved_model = load_model('best_model.keras')"]},{"cell_type":"markdown","metadata":{"id":"04kd0jgeL1pq"},"source":["**Extra Exercise**: Add the Early Stopping and Model Checkpoint callbacks to the training of the *Cats and Dogs* classification model and observe how it influences the training."]},{"cell_type":"markdown","metadata":{"id":"r2WnG8dijFcX"},"source":["## Other Datasets\n","\n","### WM-811K Dataset\n","\n","WM-811K is a semiconductor dataset containing many process issues and wafer map patterns from CP Yield、WAT(Wafer Acceptance Test) and Particle can help the engineers find some clue.\n","\n","But there is a big problem that how to classified the wafer map pattern into serveral groups without manual action. There are many papers to survey this problem, and here I will show the result of applying deep learning.\n","\n","Threre are 811,457 images in the data but only 172,950 images have labels.There are a total of 9 types of problems. From all the labeled images, the 'none' pattern occupies 85.2%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXTAMHTti258"},"outputs":[],"source":["!wget http://mirlab.org/dataSet/public/WM-811K.zip"]},{"cell_type":"markdown","metadata":{"id":"bB0Z1MiWUrhk"},"source":["### CUB-200 Dataset\n","\n","Caltech-UCSD Birds 200 (CUB-200) is an image dataset with photos of 200 bird species (mostly North American). For detailed information about the dataset, please see the technical report linked below.\n","\n","Number of categories: 200\n","\n","Number of images: 6,033\n","\n","Annotations: Bounding Box, Rough Segmentation, Attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6oiFN63JHfq"},"outputs":[],"source":["!wget http://www.vision.caltech.edu/visipedia-data/CUB-200/images.tgz"]},{"cell_type":"markdown","metadata":{"id":"tNhFD8AHU7qH"},"source":["### Food-101 Dataset\n","\n","Food-101is a challenging data set of 101 food categories, with 101,000 images. For each class, 250 manually reviewed test images are provided as well as 750 training images. On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. All images were rescaled to have a maximum side length of 512 pixels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WXSDW9dKzE6"},"outputs":[],"source":["!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}